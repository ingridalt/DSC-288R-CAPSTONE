{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c1717f9-ddc7-4a37-8b73-dfc4626f8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e52189-fbc0-4228-90c0-d08ef0d9426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#um do we include 2020??? This will load the datasets into your env btw\n",
    "# The data set is < 1GB BUTTT records wise, we have 1,928,458 which is A LOT \n",
    "# Storage shouldnt even be a problem bc our dataset isnt necessarly dense like images are \n",
    "def download_acs_1year_person_data(state_abbr=\"ca\", years=[2018,2019, 2021, 2022, 2023]):\n",
    "    \"\"\"\n",
    "    Downloads 1-Year ACS PUMS person files. \n",
    "    \"\"\"\n",
    "    for year in years:\n",
    "        url = f\"https://www2.census.gov/programs-surveys/acs/data/pums/{year}/1-Year/csv_p{state_abbr}.zip\"\n",
    "        dest_folder = f\"data_persons_{state_abbr}_1yr/{year}\"\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Downloading {year} 1-Year data...\")\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                z.extractall(dest_folder)\n",
    "                print(f\"Done: {year}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {year}: {e}\")\n",
    "\n",
    "# We only run once \n",
    "#download_acs_1year_person_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c906bcf-5ca5-4380-aa2d-1ca57fbe69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Master Table...\n",
      "data_persons_ca_1yr/2018/psam_p06.csv\n",
      "Processing Year: 2018 (CREATE TABLE master_person_data AS)...\n",
      "data_persons_ca_1yr/2019/psam_p06.csv\n",
      "Processing Year: 2019 (INSERT INTO master_person_data)...\n",
      "data_persons_ca_1yr/2021/psam_p06.csv\n",
      "Processing Year: 2021 (INSERT INTO master_person_data)...\n",
      "data_persons_ca_1yr/2022/psam_p06.csv\n",
      "Processing Year: 2022 (INSERT INTO master_person_data)...\n",
      "data_persons_ca_1yr/2023/psam_p06.csv\n",
      "Processing Year: 2023 (INSERT INTO master_person_data)...\n",
      "Aggregation Complete!\n",
      "   DATA_YEAR  person_records\n",
      "0       2018          378817\n",
      "1       2019          380091\n",
      "2       2021          386061\n",
      "3       2022          391171\n",
      "4       2023          392318\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Connect to database\n",
    "con = duckdb.connect('census_master.db')\n",
    "\n",
    "# List of downloaded yrs, should we include 2020? \n",
    "years = ['2018', '2019', '2021', '2022', '2023']\n",
    "state = '06' # CA\n",
    "\n",
    "#This isnt always best PRACTICE, but so we dont duplicate data lets keep this for now until we know what\n",
    "# db we are happy with \n",
    "con.execute(\"DROP TABLE IF EXISTS master_person_data\")\n",
    "\n",
    "print(\"Building Master Table...\")\n",
    "\n",
    "for year in years:\n",
    "    # I guess maybe we can actually update the name to make this a bit cleanr\n",
    "    p_file = f\"data_persons_ca_1yr/{year}/psam_p06.csv\"\n",
    "    print(p_file)\n",
    "    \n",
    "    if not os.path.exists(p_file):\n",
    "        print(f\"Skipping {year}: File not found at {p_path}\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.exists(p_file):\n",
    "        # Check if table already exists\n",
    "        table_exists = con.execute(\n",
    "            \"SELECT count(*) FROM information_schema.tables WHERE table_name = 'master_person_data'\"\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        # Use CREATE for the first file, INSERT for the rest\n",
    "        operation = \"INSERT INTO master_person_data\" if table_exists > 0 else \"CREATE TABLE master_person_data AS\"\n",
    "        \n",
    "        print(f\"Processing Year: {year} ({operation})...\")\n",
    "        con.execute(f\"\"\"\n",
    "            {operation}\n",
    "            SELECT \n",
    "                SERIALNO, \n",
    "                {year} as DATA_YEAR, \n",
    "                AGEP, \n",
    "                SEX, \n",
    "                SCHL, \n",
    "                WAGP,\n",
    "                ESR,\n",
    "                POVPIP\n",
    "            FROM read_csv_auto('{p_file}')\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(f\"Skipping {year}: File not found at {p_file}\")\n",
    "        \n",
    "print(\"Aggregation Complete!\")\n",
    "\n",
    "# 2. Check the final volume\n",
    "print(con.execute(\"SELECT DATA_YEAR, count(*) as person_records FROM master_person_data GROUP BY DATA_YEAR ORDER BY DATA_YEAR\").df())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f2d2e6-4642-4cdc-b43a-b0ccf36db922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------------------ These column names are the following ------------------>\n",
      "The length of Person Variables are: 286\n"
     ]
    }
   ],
   "source": [
    "all_headers_persons = pd.read_csv('data_persons_ca_1yr/2018/psam_p06.csv', nrows=0).columns.tolist()\n",
    "print(\"<------------------ These column names are the following ------------------>\")\n",
    "print(f\"The length of Person Variables are: {len(all_headers_persons)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28dcf71c-3393-4baf-8c64-d31ad25d6b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------------------ The 'Clean' column names are the following ------------------>\n",
      "RT SERIALNO DIVISION SPORDER PUMA REGION ST ADJINC AGEP CIT CITWP COW DDRS DEAR DEYE DOUT DPHY DRAT DRATX DREM ENG GCL GCM GCR HINS1 HINS2 HINS3 HINS4 HINS5 HINS6 HINS7 INTP JWMNP JWRIP JWTR LANX MAR MARHD MARHM MARHT MARHW MARHYP MIG MIL MLPA MLPB MLPCD MLPE MLPFG MLPH MLPI MLPJ MLPK NWAB NWAV NWLA NWLK NWRE OIP PAP RELP RETP SCH SCHG SCHL SEMP SEX SSIP SSP WAGP WKHP WKL WKW WRK YOEP ANC ANC1P ANC2P DECADE DIS DRIVESP ESP ESR HICOV HISP INDP JWAP JWDP LANP MIGPUMA MIGSP MSP NAICSP NATIVITY NOP OC OCCP PAOC PERNP PINCP POBP POVPIP POWPUMA POWSP PRIVCOV PUBCOV QTRBIR RAC1P RAC2P RAC3P RACAIAN RACASN RACBLK RACNH RACNUM RACPI RACSOR RACWHT RC SCIENGP SCIENGRLP SFN SFR SOCP VPS WAOB\n"
     ]
    }
   ],
   "source": [
    "#quick and dirty column clean, this is just  a list though \n",
    "clean_persons_cols = [c for c in all_headers_persons if not c.startswith('PWG') and not c.startswith('F')]\n",
    "print(\"<------------------ The 'Clean' column names are the following ------------------>\")\n",
    "print(*clean_persons_cols[:286])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3817b188-b959-42bc-a848-9fc4d43d9f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  column_name column_type null   key default extra\n",
      "0    SERIALNO     VARCHAR  YES  None    None  None\n",
      "1   DATA_YEAR     INTEGER  YES  None    None  None\n",
      "2        AGEP      BIGINT  YES  None    None  None\n",
      "3         SEX      BIGINT  YES  None    None  None\n",
      "4        SCHL     VARCHAR  YES  None    None  None\n",
      "5        WAGP      BIGINT  YES  None    None  None\n",
      "6         ESR      BIGINT  YES  None    None  None\n"
     ]
    }
   ],
   "source": [
    "columns_info = con.execute(\"DESCRIBE master_person_data\").df()\n",
    "print(columns_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc562af-9607-4d19-8582-f87d4062a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
