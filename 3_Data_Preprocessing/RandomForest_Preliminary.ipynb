{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed7bd74-b204-4448-bd8a-579130a1caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-- correcting class imablance in RF --class_weight tuning \"balance_subsample\"?\n",
    "-- explore features outside of LR model\n",
    "-- variable mapping\n",
    "-- verify null handling for each\n",
    "-- RC related child variable might be all NaN bc it is N/A to our people. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8f635",
   "metadata": {},
   "source": [
    "# Random Forest Model (ACS Poverty Risk) — Preprocessing + Full OCCP\n",
    "\n",
    "This notebook mirrors some of the baseline logistic-regression preprocessing, but adapts it for a tree model:\n",
    "\n",
    "- **No scaling**\n",
    "- **No top-10 OCCP restriction** (uses all occupation codes via one-hot encoding)\n",
    "- **Avoids treating valid `0` values as missing**\n",
    "- Uses **imputation** to handle true missing values\n",
    "- Trains a **RandomForestClassifier** with `class_weight=\"balanced\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39977c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db192920",
   "metadata": {},
   "source": [
    "## 1) Load train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d490c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1857626, 26)\n",
      "Test shape : (378571, 103)\n",
      "\n",
      "Target distribution (train):\n",
      "poverty_risk_score\n",
      "0.0    1375161\n",
      "1.0     268696\n",
      "2.0     109241\n",
      "3.0     104528\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#paths taken from baseline model\n",
    "train_path = \"preprocessing_data/train_data_final_feat_no_preprocessing.csv\"\n",
    "test_path  = \"preprocessing_data/test_data_final_feat_no_preprocessing.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape :\", df_test.shape)\n",
    "print(\"\\nTarget distribution (train):\")\n",
    "print(df_train[\"poverty_risk_score\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74ee7e",
   "metadata": {},
   "source": [
    "## 2) Preprocessing for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9caa0808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_acs_data_rf(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess ACS-like features for Random Forest.\n",
    "\n",
    "    Key choices:\n",
    "    - Do NOT fill binary columns with 0 (0 can be meaningful).\n",
    "    - Map {1,2} survey binaries -> {1,0} while leaving NaN as NaN.\n",
    "    - Keep full OCCP (no top-10 grouping).\n",
    "    - Keep correlated features (RF doesn't require multicollinearity pruning).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Binary recoding (leave NaN as NaN) ---\n",
    "    mapping_12 = {1: 1, 2: 0}  # 1=yes, 2=no\n",
    "\n",
    "    # insurance + disability + sex\n",
    "    for col in [\"PRIVCOV\", \"PUBCOV\", \"DIS\", \"SEX\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping_12)\n",
    "\n",
    "    # HICOV: {1:0, 2:1} where 2 indicates \"No insurance\"\n",
    "    if \"HICOV\" in df.columns:\n",
    "        df[\"HICOV\"] = df[\"HICOV\"].map({1: 0, 2: 1})\n",
    "\n",
    "    # MAR: treat only explicit \"1\" as married; keep NaNs\n",
    "    if \"MAR\" in df.columns:\n",
    "        df[\"MAR\"] = np.where(df[\"MAR\"].isna(), np.nan, (df[\"MAR\"] == 1).astype(int))\n",
    "\n",
    "    # --- Feature engineering / recodes ---\n",
    "    if \"CIT\" in df.columns:\n",
    "        df[\"CIT\"] = np.where(df[\"CIT\"].isna(), np.nan, (df[\"CIT\"] < 5).astype(int))\n",
    "\n",
    "    # Employment indicators (keep as binaries rather than collapsing everything)\n",
    "    if \"ESR\" in df.columns:\n",
    "        df[\"ESR_emp\"] = np.where(df[\"ESR\"].isna(), np.nan, df[\"ESR\"].isin([1, 2]).astype(int))\n",
    "\n",
    "    if \"WRK\" in df.columns:\n",
    "        # ACS often uses 1=yes 2=no (confirm in your codebook)\n",
    "        df[\"WRK\"] = df[\"WRK\"].map(mapping_12)\n",
    "\n",
    "    if \"WKL\" in df.columns:\n",
    "        # WKL is typically categorical/ordinal (\"when last worked\") – keep as category later\n",
    "        # Don't recode here unless you have a specific mapping\n",
    "        pass\n",
    "\n",
    "    if \"MIG\" in df.columns:\n",
    "        df[\"MIG_recent\"] = np.where(df[\"MIG\"].isna(), np.nan, df[\"MIG\"].isin([2, 3]).astype(int))\n",
    "\n",
    "    # Nativity: many ACS extracts use 1=native, 2=foreign born (confirm); map to binary foreign-born flag\n",
    "    if \"NATIVITY\" in df.columns:\n",
    "        df[\"ForeignBorn\"] = df[\"NATIVITY\"].map({1: 0, 2: 1})\n",
    "\n",
    "    # Place of birth: keep original and engineer Born_in_CA\n",
    "    if \"POBP\" in df.columns:\n",
    "        df[\"Born_in_CA\"] = np.where(df[\"POBP\"].isna(), np.nan, (df[\"POBP\"] == 6).astype(int))\n",
    "\n",
    "    # Language features\n",
    "    if \"LANX\" in df.columns:\n",
    "        df[\"LANX\"] = np.where(df[\"LANX\"].isna(), np.nan, (df[\"LANX\"] == 1).astype(int))\n",
    "\n",
    "    if \"ENG\" in df.columns:\n",
    "        # ENG is usually ordinal (ability to speak English). Keep raw; you can bucket later if you want.\n",
    "        pass\n",
    "\n",
    "    if \"LANP\" in df.columns:\n",
    "        # LANP is language code; keep raw categorical\n",
    "        pass\n",
    "\n",
    "    # Education tiers\n",
    "    if \"SCHL\" in df.columns:\n",
    "        def recode_education(val):\n",
    "            if pd.isna(val) or val <= 15: return 0\n",
    "            if val <= 17: return 1\n",
    "            if val <= 20: return 2\n",
    "            return 3\n",
    "        df[\"SCHL_Tier\"] = df[\"SCHL\"].apply(recode_education)\n",
    "\n",
    "    # --- Treat as categorical for one-hot encoding (later) ---\n",
    "    for cat_col in [\"OCCP\", \"CA_Region\", \"RAC1P\", \"year\", \"MSP\", \"WKL\", \"ENG\", \"LANP\", \"PUMA\", \"POBP\"]:\n",
    "        if cat_col in df.columns:\n",
    "            df[cat_col] = df[cat_col].astype(\"string\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c567946a-7c3c-4030-a164-05a13683cce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique OCCP codes: 530\n",
      "\n",
      "Top 10 most common:\n",
      "OCCP\n",
      "440.0     32169\n",
      "4720.0    23449\n",
      "4760.0    22976\n",
      "9130.0    21427\n",
      "2310.0    20675\n",
      "1021.0    19929\n",
      "3255.0    19870\n",
      "3602.0    19829\n",
      "5240.0    18304\n",
      "4700.0    17148\n",
      "Name: count, dtype: int64\n",
      "\n",
      "How many OCCP values appear only once?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique OCCP codes:\", df_train[\"OCCP\"].nunique())\n",
    "\n",
    "print(\"\\nTop 10 most common:\")\n",
    "print(df_train[\"OCCP\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nHow many OCCP values appear only once?\")\n",
    "print((df_train[\"OCCP\"].value_counts() == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56f4cffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing.\n"
     ]
    }
   ],
   "source": [
    "df_train_pp = preprocess_acs_data_rf(df_train)\n",
    "df_test_pp  = preprocess_acs_data_rf(df_test)\n",
    "\n",
    "print(\"Done preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "313ce2bd-a6f4-46c5-9daf-bf494f806504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCCP unique values: 530\n",
      "RAC1P unique values: 9\n",
      "CA_Region unique values: 7\n",
      "year unique values: 5\n"
     ]
    }
   ],
   "source": [
    "for col in [\"OCCP\", \"RAC1P\", \"CA_Region\", \"year\"]:\n",
    "    print(col, \"unique values:\", df_train_pp[col].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6939a",
   "metadata": {},
   "source": [
    "## 3) Choose features + one-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb02e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sparse shape: (1857626, 559)\n",
      "X_test_sparse shape : (378571, 559)\n",
      "Sparse matrix type: <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Features aligned to your baseline notebook, with OCCP (full, ungrouped)\n",
    "features = [\n",
    "    \"AGEP\", \"WKHP\", \"SEX\", \"DIS\", \"CIT\", \"Born_in_CA\",\n",
    "    \"SCHL_Tier\", \"OCCP\", \"CA_Region\", \"RAC1P\", \"year\"\n",
    "]\n",
    "\n",
    "missing_feats = [c for c in features if c not in df_train_pp.columns]\n",
    "if missing_feats:\n",
    "    print(\"WARNING: These features are missing from train:\", missing_feats)\n",
    "\n",
    "# split your features\n",
    "num_features = [\"AGEP\", \"WKHP\", \"SCHL_Tier\"]\n",
    "bin_features = [\"SEX\", \"DIS\", \"CIT\", \"Born_in_CA\"]   # already 0/1 but may have NaNs\n",
    "cat_features = [\"OCCP\", \"CA_Region\", \"RAC1P\", \"year\"]\n",
    "\n",
    "# Buckets ultra-rare categories into an \"infrequent\" bin (helps if any category is very high-cardinality)\n",
    "# With your nunique counts, this won't change much, but it's safe.\n",
    "MIN_FREQ = 5\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_features),\n",
    "        (\"bin\", SimpleImputer(strategy=\"most_frequent\"), bin_features),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                sparse_output=True,\n",
    "                min_frequency=MIN_FREQ\n",
    "            ))\n",
    "        ]), cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# --- Build X/y ---\n",
    "X_train = df_train_pp[features].copy()\n",
    "X_test  = df_test_pp[features].copy()\n",
    "\n",
    "y_train = df_train_pp[\"poverty_risk_score\"].astype(int)\n",
    "y_test  = df_test_pp[\"poverty_risk_score\"].astype(int)\n",
    "\n",
    "# --- IMPORTANT FIX: remove pandas pd.NA ambiguity for sklearn ---\n",
    "# 1) Ensure numeric-ish columns are real numeric with np.nan (not pd.NA)\n",
    "for c in num_features + bin_features:\n",
    "    X_train[c] = pd.to_numeric(X_train[c], errors=\"coerce\")\n",
    "    X_test[c]  = pd.to_numeric(X_test[c], errors=\"coerce\")\n",
    "\n",
    "# 2) Ensure categorical columns are plain Python objects, and missing is None (not pd.NA)\n",
    "for c in cat_features:\n",
    "    X_train[c] = X_train[c].astype(\"object\")\n",
    "    X_test[c]  = X_test[c].astype(\"object\")\n",
    "    X_train[c] = X_train[c].where(pd.notna(X_train[c]), None)\n",
    "    X_test[c]  = X_test[c].where(pd.notna(X_test[c]), None)\n",
    "\n",
    "# --- Fit/transform train, transform test ---\n",
    "X_train_sparse = preprocess.fit_transform(X_train)\n",
    "X_test_sparse  = preprocess.transform(X_test)\n",
    "\n",
    "print(\"X_train_sparse shape:\", X_train_sparse.shape)\n",
    "print(\"X_test_sparse shape :\", X_test_sparse.shape)\n",
    "print(\"Sparse matrix type:\", type(X_train_sparse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f46d113-8ec3-4c26-bed3-9570109222ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.61      0.72    285368\n",
      "           1       0.22      0.33      0.26     51329\n",
      "           2       0.12      0.28      0.16     20679\n",
      "           3       0.15      0.36      0.21     21195\n",
      "\n",
      "    accuracy                           0.54    378571\n",
      "   macro avg       0.34      0.40      0.34    378571\n",
      "weighted avg       0.71      0.54      0.60    378571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=250,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    min_samples_leaf=10,\n",
    "    min_samples_split=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "rf.fit(X_train_sparse, y_train)\n",
    "y_pred = rf.predict(X_test_sparse)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c30fc2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[175045  52064  29163  29096]\n",
      " [ 14041  17074  11158   9056]\n",
      " [  4657   5655   5854   4513]\n",
      " [  4926   4292   4396   7581]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946bc04",
   "metadata": {},
   "source": [
    "## 6) Feature importances (top 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5944722d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (559) does not match length of index (28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Feature importance can be noisy with correlated / high-cardinality OHE.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Still useful for quick sanity checks.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train_pp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m display(importances\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m25\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Optional quick plot\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:503\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    501\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 503\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (559) does not match length of index (28)"
     ]
    }
   ],
   "source": [
    "# Feature importance can be noisy with correlated / high-cardinality OHE.\n",
    "# Still useful for quick sanity checks.\n",
    "importances = pd.Series(rf.feature_importances_, index=df_train_pp.columns).sort_values(ascending=False)\n",
    "\n",
    "display(importances.head(25))\n",
    "\n",
    "# Optional quick plot\n",
    "top_n = 25\n",
    "top = importances.head(top_n).sort_values()\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.barh(top.index, top.values)\n",
    "plt.title(f\"Top {top_n} Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f6c58",
   "metadata": {},
   "source": [
    "## 7) Optional: Quick train/validation split for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3da251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want a fast local check without full CV:\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_df, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "X_tr_i = imputer.fit_transform(X_tr)\n",
    "X_val_i = imputer.transform(X_val)\n",
    "\n",
    "rf_quick = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_quick.fit(X_tr_i, y_tr)\n",
    "val_pred = rf_quick.predict(X_val_i)\n",
    "\n",
    "print(\"=== Quick Validation Performance ===\")\n",
    "print(classification_report(y_val, val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c17a13-1361-43b7-b607-9561292aba3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
