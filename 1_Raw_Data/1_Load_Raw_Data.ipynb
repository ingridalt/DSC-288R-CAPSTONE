{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627b81a7-5c5d-4d0d-affb-8047aee727da",
   "metadata": {},
   "source": [
    "# Obtaining Raw Data \n",
    "This project uses the American Community Survey (ACS) PUMS 1-Year national person-level microdata\n",
    "i.e. Census Data\n",
    "2018â€“2023 (excluding 2020 due to the unusual circumstances of the pandemic)\n",
    "\n",
    "Due to size constraints (>10GB raw CSVs), the raw datasets are not tracked in GitHub\n",
    "<br>\n",
    "All data can be fully reproduced by running the script, more detail on reproducibility in README file\n",
    "\n",
    "Raw data source:\n",
    "<br>\n",
    "https://www.census.gov/programs-surveys/acs/microdata.html\n",
    "<br>\n",
    "https://www2.census.gov/programs-surveys/acs/data/pums/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab24d8a-e26a-4da3-b1c2-6e5c1f09d99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91acada1-28f3-490c-89ad-ab9a5f6cc1e3",
   "metadata": {},
   "source": [
    "# CA Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a904408a-c407-410a-bf33-020254810b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6541caa5-b505-4428-904b-a1225e8254b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2018 1-Year data...\n",
      "Done: 2018\n",
      "Downloading 2019 1-Year data...\n",
      "Done: 2019\n",
      "Downloading 2021 1-Year data...\n",
      "Done: 2021\n",
      "Downloading 2022 1-Year data...\n",
      "Done: 2022\n",
      "Downloading 2023 1-Year data...\n",
      "Done: 2023\n"
     ]
    }
   ],
   "source": [
    "def download_acs_1year_person_data(state_abbr=\"ca\", years=[2018,2019, 2021, 2022, 2023]):\n",
    "    \"\"\"\n",
    "    Downloads 1-Year ACS PUMS person files. \n",
    "    \"\"\"\n",
    "    for year in years:\n",
    "        url = f\"https://www2.census.gov/programs-surveys/acs/data/pums/{year}/1-Year/csv_p{state_abbr}.zip\"\n",
    "        dest_folder = f\"data_persons_{state_abbr}_1yr/{year}\"\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Downloading {year} 1-Year data...\")\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                z.extractall(dest_folder)\n",
    "                print(f\"Done: {year}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {year}: {e}\")\n",
    "\n",
    "# We only run once \n",
    "download_acs_1year_person_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5c845a-a944-4bef-ad30-207907b97c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns across 5 years: 277\n",
      "Loaded 2018: 378,817 rows, 278 cols (incl year)\n",
      "Loaded 2019: 380,091 rows, 278 cols (incl year)\n",
      "Loaded 2021: 386,061 rows, 278 cols (incl year)\n",
      "Loaded 2022: 391,171 rows, 278 cols (incl year)\n",
      "Loaded 2023: 392,318 rows, 278 cols (incl year)\n",
      "Master shape: 1,928,458 rows, 278 cols\n",
      "Saved -> data_persons_ca_1yr/psam_p06_master.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"data_persons_ca_1yr\"\n",
    "YEARS = [2018, 2019, 2021, 2022, 2023]\n",
    "FILENAME = \"psam_p06.csv\"\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"psam_p06_master.csv\")\n",
    "\n",
    "def build_master_common_columns(base_dir=BASE_DIR, years=YEARS, filename=FILENAME, output_path=OUTPUT_PATH):\n",
    "    # 1) Collect file paths + compute the intersection of column names across years\n",
    "    paths = {}\n",
    "    common_cols = None\n",
    "\n",
    "    for y in years:\n",
    "        path = os.path.join(base_dir, str(y), filename)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing file for {y}: {path}\")\n",
    "        paths[y] = path\n",
    "\n",
    "        # Read only header to get columns (fast)\n",
    "        cols = pd.read_csv(path, nrows=0).columns\n",
    "        cols_set = set(cols)\n",
    "\n",
    "        if common_cols is None:\n",
    "            common_cols = cols_set\n",
    "        else:\n",
    "            common_cols = common_cols.intersection(cols_set)\n",
    "\n",
    "    # We will keep these columns (sorted for stable order) + add \"year\"\n",
    "    common_cols = sorted(common_cols)\n",
    "    if \"year\" in common_cols:\n",
    "        # unlikely, but just in case\n",
    "        common_cols.remove(\"year\")\n",
    "\n",
    "    print(f\"Common columns across {len(years)} years: {len(common_cols)}\")\n",
    "\n",
    "    # 2) Read each year using only the common columns, add year, concat\n",
    "    dfs = []\n",
    "    for y in years:\n",
    "        df = pd.read_csv(\n",
    "            paths[y],\n",
    "            usecols=common_cols,     # ensures matching schema\n",
    "            low_memory=False\n",
    "        )\n",
    "        df[\"year\"] = y\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {y}: {df.shape[0]:,} rows, {df.shape[1]} cols (incl year)\")\n",
    "\n",
    "    master = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Master shape: {master.shape[0]:,} rows, {master.shape[1]} cols\")\n",
    "\n",
    "    # 3) Write out\n",
    "    master.to_csv(output_path, index=False)\n",
    "    print(f\"Saved -> {output_path}\")\n",
    "\n",
    "    return master, common_cols\n",
    "\n",
    "master_df, common_cols = build_master_common_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd382c2-e9c5-4546-9bee-a61d62897e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1,928,458\n",
      "Columns: 278\n",
      "On-disk size: 1.37 GB\n"
     ]
    }
   ],
   "source": [
    "# Rows & columns\n",
    "print(f\"Rows: {master_df.shape[0]:,}\")\n",
    "print(f\"Columns: {master_df.shape[1]}\")\n",
    "\n",
    "import os\n",
    "\n",
    "path = \"data_persons_ca_1yr/psam_p06_master.csv\"\n",
    "\n",
    "size_bytes = os.path.getsize(path)\n",
    "size_gb = size_bytes / 1024**3\n",
    "\n",
    "print(f\"On-disk size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ff1c0-f7c8-42bd-866f-09ea9cc7c312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a110083-b7a6-45db-b282-5b993fd199e2",
   "metadata": {},
   "source": [
    "# USA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92bbd22-509c-4356-a62b-8b9b14415717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2018 US 1-Year person data...\n",
      "Done: 2018 -> data_persons_us_1yr/2018\n",
      "Downloading 2019 US 1-Year person data...\n",
      "Done: 2019 -> data_persons_us_1yr/2019\n",
      "Downloading 2021 US 1-Year person data...\n",
      "Done: 2021 -> data_persons_us_1yr/2021\n",
      "Downloading 2022 US 1-Year person data...\n",
      "Done: 2022 -> data_persons_us_1yr/2022\n",
      "Downloading 2023 US 1-Year person data...\n",
      "Done: 2023 -> data_persons_us_1yr/2023\n"
     ]
    }
   ],
   "source": [
    "def download_acs_1yr_us_person(years, base_dir=\"data_persons_us_1yr\"):\n",
    "    for year in years:\n",
    "        url = f\"https://www2.census.gov/programs-surveys/acs/data/pums/{year}/1-Year/csv_pus.zip\"\n",
    "        dest = Path(base_dir) / str(year)\n",
    "        dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"Downloading {year} US 1-Year person data...\")\n",
    "        try:\n",
    "            r = requests.get(url, timeout=120)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                z.extractall(dest)\n",
    "\n",
    "            print(f\"Done: {year} -> {dest}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {year}: {e}\")\n",
    "\n",
    "YEARS = [2018, 2019, 2021, 2022, 2023]\n",
    "download_acs_1yr_us_person(YEARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ba9b2b-465f-4775-8636-0a40bbfb14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns across 5 years (all parts): 277\n",
      "\n",
      "Processing 2018 from psam_pusa.csv ...\n",
      "Done 2018 psam_pusa.csv. Total rows written so far: 1,648,512\n",
      "\n",
      "Processing 2018 from psam_pusb.csv ...\n",
      "Done 2018 psam_pusb.csv. Total rows written so far: 3,214,539\n",
      "\n",
      "Processing 2019 from psam_pusa.csv ...\n",
      "Done 2019 psam_pusa.csv. Total rows written so far: 4,873,347\n",
      "\n",
      "Processing 2019 from psam_pusb.csv ...\n",
      "Done 2019 psam_pusb.csv. Total rows written so far: 6,454,092\n",
      "\n",
      "Processing 2021 from psam_pusa.csv ...\n",
      "Done 2021 psam_pusa.csv. Total rows written so far: 8,123,471\n",
      "\n",
      "Processing 2021 from psam_pusb.csv ...\n",
      "Done 2021 psam_pusb.csv. Total rows written so far: 9,706,691\n",
      "\n",
      "Processing 2022 from psam_pusa.csv ...\n",
      "Done 2022 psam_pusa.csv. Total rows written so far: 11,430,845\n",
      "\n",
      "Processing 2022 from psam_pusb.csv ...\n",
      "Done 2022 psam_pusb.csv. Total rows written so far: 13,080,069\n",
      "\n",
      "Processing 2023 from psam_pusa.csv ...\n",
      "Done 2023 psam_pusa.csv. Total rows written so far: 14,812,412\n",
      "\n",
      "Processing 2023 from psam_pusb.csv ...\n",
      "Done 2023 psam_pusb.csv. Total rows written so far: 16,485,878\n",
      "\n",
      "Saved -> data_persons_us_1yr/psam_pus_master.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"data_persons_us_1yr\")\n",
    "YEARS = [2018, 2019, 2021, 2022, 2023]\n",
    "OUTPUT_PATH = BASE_DIR / \"psam_pus_master.csv\"\n",
    "\n",
    "def find_person_csvs_for_year(year_folder: Path):\n",
    "    \"\"\"\n",
    "    For US data, Census splits the national person file into parts (A/B),\n",
    "    e.g., psam_pusa.csv and psam_pusb.csv. This returns BOTH (or all) parts.\n",
    "    \"\"\"\n",
    "    candidates = sorted(year_folder.glob(\"psam_pus*.csv\"))  # grabs pusa/pusb etc.\n",
    "    if not candidates:\n",
    "        # fallback just in case naming differs\n",
    "        candidates = sorted(year_folder.glob(\"psam_p*.csv\"))\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No person CSVs found in {year_folder}\")\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def get_common_columns_across_all_files(files):\n",
    "    common = None\n",
    "    for path in files:\n",
    "        cols = pd.read_csv(path, nrows=0).columns\n",
    "        common = set(cols) if common is None else common.intersection(set(cols))\n",
    "\n",
    "    common = sorted(common)\n",
    "    if \"year\" in common:\n",
    "        common.remove(\"year\")\n",
    "    return common\n",
    "\n",
    "def build_master_us_streaming(base_dir=BASE_DIR, years=YEARS, output_path=OUTPUT_PATH, chunksize=200_000):\n",
    "    # 1) Build list of all input files (includes A/B per year)\n",
    "    files_by_year = {}\n",
    "    all_files = []\n",
    "\n",
    "    for y in years:\n",
    "        year_folder = base_dir / str(y)\n",
    "        year_files = find_person_csvs_for_year(year_folder)\n",
    "        files_by_year[y] = year_files\n",
    "        all_files.extend(year_files)\n",
    "\n",
    "    # 2) Compute common columns across ALL files (all years, all parts)\n",
    "    common_cols = get_common_columns_across_all_files(all_files)\n",
    "    print(f\"Common columns across {len(years)} years (all parts): {len(common_cols)}\")\n",
    "\n",
    "    # 3) Write streaming\n",
    "    if output_path.exists():\n",
    "        output_path.unlink()\n",
    "\n",
    "    wrote_header = False\n",
    "    total_rows = 0\n",
    "\n",
    "    for y in years:\n",
    "        for path in files_by_year[y]:\n",
    "            print(f\"\\nProcessing {y} from {path.name} ...\")\n",
    "\n",
    "            for chunk in pd.read_csv(path, usecols=common_cols, chunksize=chunksize, low_memory=False):\n",
    "                chunk[\"year\"] = y\n",
    "                total_rows += len(chunk)\n",
    "\n",
    "                chunk.to_csv(\n",
    "                    output_path,\n",
    "                    mode=\"a\",\n",
    "                    index=False,\n",
    "                    header=not wrote_header\n",
    "                )\n",
    "                wrote_header = True\n",
    "\n",
    "            print(f\"Done {y} {path.name}. Total rows written so far: {total_rows:,}\")\n",
    "\n",
    "    print(f\"\\nSaved -> {output_path}\")\n",
    "    return common_cols\n",
    "\n",
    "common_cols = build_master_us_streaming()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f171ef-80df-4c11-8575-da5f1c271eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 16,485,878\n",
      "Columns: 278\n",
      "On-disk size: 11.62 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"data_persons_us_1yr/psam_pus_master.csv\"\n",
    "\n",
    "# Columns (read header only)\n",
    "n_cols = len(pd.read_csv(path, nrows=0).columns)\n",
    "\n",
    "# Rows (fast-ish line count; subtract header)\n",
    "with open(path, \"rb\") as f:\n",
    "    n_rows = sum(1 for _ in f) - 1\n",
    "\n",
    "size_gb = os.path.getsize(path) / 1024**3\n",
    "\n",
    "print(f\"Rows: {n_rows:,}\")\n",
    "print(f\"Columns: {n_cols}\")\n",
    "print(f\"On-disk size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c583e2-1e2e-4987-839c-c1cdfcdc00c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
