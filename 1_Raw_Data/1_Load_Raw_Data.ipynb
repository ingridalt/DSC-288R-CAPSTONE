{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a904408a-c407-410a-bf33-020254810b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6541caa5-b505-4428-904b-a1225e8254b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2018 1-Year data...\n",
      "Done: 2018\n",
      "Downloading 2019 1-Year data...\n",
      "Done: 2019\n",
      "Downloading 2021 1-Year data...\n",
      "Done: 2021\n",
      "Downloading 2022 1-Year data...\n",
      "Done: 2022\n",
      "Downloading 2023 1-Year data...\n",
      "Done: 2023\n"
     ]
    }
   ],
   "source": [
    "def download_acs_1year_person_data(state_abbr=\"ca\", years=[2018,2019, 2021, 2022, 2023]):\n",
    "    \"\"\"\n",
    "    Downloads 1-Year ACS PUMS person files. \n",
    "    \"\"\"\n",
    "    for year in years:\n",
    "        url = f\"https://www2.census.gov/programs-surveys/acs/data/pums/{year}/1-Year/csv_p{state_abbr}.zip\"\n",
    "        dest_folder = f\"data_persons_{state_abbr}_1yr/{year}\"\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"Downloading {year} 1-Year data...\")\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "                z.extractall(dest_folder)\n",
    "                print(f\"Done: {year}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {year}: {e}\")\n",
    "\n",
    "# We only run once \n",
    "download_acs_1year_person_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5c845a-a944-4bef-ad30-207907b97c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns across 5 years: 277\n",
      "Loaded 2018: 378,817 rows, 278 cols (incl year)\n",
      "Loaded 2019: 380,091 rows, 278 cols (incl year)\n",
      "Loaded 2021: 386,061 rows, 278 cols (incl year)\n",
      "Loaded 2022: 391,171 rows, 278 cols (incl year)\n",
      "Loaded 2023: 392,318 rows, 278 cols (incl year)\n",
      "Master shape: 1,928,458 rows, 278 cols\n",
      "Saved -> data_persons_ca_1yr/psam_p06_master.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"data_persons_ca_1yr\"\n",
    "YEARS = [2018, 2019, 2021, 2022, 2023]\n",
    "FILENAME = \"psam_p06.csv\"\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"psam_p06_master.csv\")\n",
    "\n",
    "def build_master_common_columns(base_dir=BASE_DIR, years=YEARS, filename=FILENAME, output_path=OUTPUT_PATH):\n",
    "    # 1) Collect file paths + compute the intersection of column names across years\n",
    "    paths = {}\n",
    "    common_cols = None\n",
    "\n",
    "    for y in years:\n",
    "        path = os.path.join(base_dir, str(y), filename)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing file for {y}: {path}\")\n",
    "        paths[y] = path\n",
    "\n",
    "        # Read only header to get columns (fast)\n",
    "        cols = pd.read_csv(path, nrows=0).columns\n",
    "        cols_set = set(cols)\n",
    "\n",
    "        if common_cols is None:\n",
    "            common_cols = cols_set\n",
    "        else:\n",
    "            common_cols = common_cols.intersection(cols_set)\n",
    "\n",
    "    # We will keep these columns (sorted for stable order) + add \"year\"\n",
    "    common_cols = sorted(common_cols)\n",
    "    if \"year\" in common_cols:\n",
    "        # unlikely, but just in case\n",
    "        common_cols.remove(\"year\")\n",
    "\n",
    "    print(f\"Common columns across {len(years)} years: {len(common_cols)}\")\n",
    "\n",
    "    # 2) Read each year using only the common columns, add year, concat\n",
    "    dfs = []\n",
    "    for y in years:\n",
    "        df = pd.read_csv(\n",
    "            paths[y],\n",
    "            usecols=common_cols,     # ensures matching schema\n",
    "            low_memory=False\n",
    "        )\n",
    "        df[\"year\"] = y\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {y}: {df.shape[0]:,} rows, {df.shape[1]} cols (incl year)\")\n",
    "\n",
    "    master = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Master shape: {master.shape[0]:,} rows, {master.shape[1]} cols\")\n",
    "\n",
    "    # 3) Write out\n",
    "    master.to_csv(output_path, index=False)\n",
    "    print(f\"Saved -> {output_path}\")\n",
    "\n",
    "    return master, common_cols\n",
    "\n",
    "master_df, common_cols = build_master_common_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acd382c2-e9c5-4546-9bee-a61d62897e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1,928,458\n",
      "Columns: 278\n",
      "On-disk size: 1.37 GB\n"
     ]
    }
   ],
   "source": [
    "# Rows & columns\n",
    "print(f\"Rows: {master_df.shape[0]:,}\")\n",
    "print(f\"Columns: {master_df.shape[1]}\")\n",
    "\n",
    "import os\n",
    "\n",
    "path = \"data_persons_ca_1yr/psam_p06_master.csv\"\n",
    "\n",
    "size_bytes = os.path.getsize(path)\n",
    "size_gb = size_bytes / 1024**3\n",
    "\n",
    "print(f\"On-disk size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ff1c0-f7c8-42bd-866f-09ea9cc7c312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06538fdd-05cb-4fdf-8f79-c25ed926cb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b467dd-fd97-4037-9b6b-360c2681973d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24511efc-78c9-4996-bea3-1f822406f357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bbd22-509c-4356-a62b-8b9b14415717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_acs_1yr_us_person(years, base_dir=\"data_persons_us_1yr\"):\n",
    "#     for year in years:\n",
    "#         url = f\"https://www2.census.gov/programs-surveys/acs/data/pums/{year}/1-Year/csv_pus.zip\"\n",
    "#         dest = Path(base_dir) / str(year)\n",
    "#         dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         print(f\"Downloading {year} US 1-Year person data...\")\n",
    "#         try:\n",
    "#             r = requests.get(url, timeout=120)\n",
    "#             r.raise_for_status()\n",
    "\n",
    "#             with zipfile.ZipFile(io.BytesIO(r.content)) as z:\n",
    "#                 z.extractall(dest)\n",
    "\n",
    "#             print(f\"Done: {year} -> {dest}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Skipping {year}: {e}\")\n",
    "\n",
    "# YEARS = [2018, 2019, 2021, 2022, 2023]\n",
    "# download_acs_1yr_us_person(YEARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba9b2b-465f-4775-8636-0a40bbfb14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# BASE_DIR = Path(\"data_persons_us_1yr\")\n",
    "# YEARS = [2018, 2019, 2021, 2022, 2023]\n",
    "# OUTPUT_PATH = BASE_DIR / \"psam_pus_master.csv\"\n",
    "\n",
    "# def find_person_csv(year_folder: Path) -> Path:\n",
    "#     # Find the big person file inside the extracted zip (usually starts with psam_p)\n",
    "#     candidates = list(year_folder.glob(\"psam_p*.csv\"))\n",
    "#     if not candidates:\n",
    "#         raise FileNotFoundError(f\"No psam_p*.csv found in {year_folder}\")\n",
    "#     # pick the largest (safest if there are multiple)\n",
    "#     return max(candidates, key=lambda p: p.stat().st_size)\n",
    "\n",
    "# def get_common_columns(paths_by_year):\n",
    "#     common = None\n",
    "#     for y, path in paths_by_year.items():\n",
    "#         cols = pd.read_csv(path, nrows=0).columns\n",
    "#         common = set(cols) if common is None else common.intersection(set(cols))\n",
    "#     common = sorted(common)\n",
    "#     if \"year\" in common:\n",
    "#         common.remove(\"year\")\n",
    "#     return common\n",
    "\n",
    "# def build_master_us_streaming(base_dir=BASE_DIR, years=YEARS, output_path=OUTPUT_PATH, chunksize=200_000):\n",
    "#     # 1) Resolve actual CSV paths for each year\n",
    "#     paths = {}\n",
    "#     for y in years:\n",
    "#         year_folder = base_dir / str(y)\n",
    "#         paths[y] = find_person_csv(year_folder)\n",
    "\n",
    "#     # 2) Compute common columns\n",
    "#     common_cols = get_common_columns(paths)\n",
    "#     print(f\"Common columns across {len(years)} years: {len(common_cols)}\")\n",
    "\n",
    "#     # 3) Write streaming\n",
    "#     if output_path.exists():\n",
    "#         output_path.unlink()\n",
    "\n",
    "#     wrote_header = False\n",
    "#     total_rows = 0\n",
    "\n",
    "#     for y in years:\n",
    "#         print(f\"\\nProcessing {y} from {paths[y].name} ...\")\n",
    "#         for chunk in pd.read_csv(paths[y], usecols=common_cols, chunksize=chunksize, low_memory=False):\n",
    "#             chunk[\"year\"] = y\n",
    "#             total_rows += len(chunk)\n",
    "#             chunk.to_csv(output_path, mode=\"a\", index=False, header=not wrote_header)\n",
    "#             wrote_header = True\n",
    "#         print(f\"Done {y}. Total rows written so far: {total_rows:,}\")\n",
    "\n",
    "#     print(f\"\\nSaved -> {output_path}\")\n",
    "#     return common_cols\n",
    "\n",
    "# common_cols = build_master_us_streaming()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
